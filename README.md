<div align="center">
<h1>Augmenting Continual Learning of Diseases with LLM-Generated Visual Concepts</h1>
</div>

<div align="center">
<p>
    <a href="">Jiantao Tan</a><sup>1</sup>&nbsp;&nbsp;
    <a href="">Peixian Ma</a><sup>2</sup>&nbsp;&nbsp;
    <a href="">Kanghao Chen</a><sup>2</sup>&nbsp;&nbsp;
    <a href="">Zhiming Dai</a><sup>1</sup>&nbsp;&nbsp;
    <a href="">Ruixuan Wang</a><sup>1,3,4</sup>&nbsp;&nbsp;
</p>

<p>
    <sup>1</sup>Sun Yat-sen University
    <sup>2</sup>The Hong Kong University of Science and Technology (Guangzhou)
    <sup>3</sup>Peng Cheng Laboratory
    <sup>4</sup>Key Laboratory of Machine Intelligence and Advanced Computing
</p>
</div>


<div align="center" style="display: flex; gap: 5px; justify-content: center;">
<a href="https://arxiv.org/abs/2508.03094"><img src="https://img.shields.io/badge/arXiv-red?style=for-the-badge&logo=arxiv"/></a>
<a href="https://github.com/MPX0222/VisualConcepts4CL"><img src="https://img.shields.io/badge/GitHub-black?style=for-the-badge&logo=github"/></a>
<a href="https://github.com/MPX0222/VisualConcepts4CL/stargazers"><img src="https://img.shields.io/github/stars/MPX0222/VisualConcepts4CL?style=for-the-badge&color=white"/></a>
</div>

---

## üìñ Abstract

Continual learning is essential for medical image classification systems to adapt to dynamically evolving clinical environments. The integration of multimodal information can significantly enhance continual learning of image classes. However, while existing approaches do utilize textual modality information, they solely rely on simplistic templates with a class name, thereby neglecting richer semantic information. To address these limitations, we propose a novel framework that harnesses visual concepts generated by large language models (LLMs) as discriminative semantic guidance. Our method dynamically constructs a visual concept pool with a similarity-based filtering mechanism to prevent redundancy. Then, to integrate the concepts into the continual learning process, we employ a cross-modal image-concept attention module, coupled with an attention loss. Through attention, the module can leverage the semantic knowledge from relevant visual concepts and produce  class-representative fused features for classification. Experiments on medical and natural image datasets show our method achieves state-of-the-art performance, demonstrating the effectiveness and superiority of our method. We will release the code publicly.

## üìù Citation

If you find this work useful, please cite:

```bibtex
@article{tan2024augmenting,
  title={Augmenting Continual Learning of Diseases with LLM-Generated Visual Concepts},
  author={Tan, Jiantao and Ma, Peixian and Chen, Kanghao and Dai, Zhiming and Wang, Ruixuan},
  journal={arXiv preprint arXiv:2508.03094},
  year={2024}
}
```

## üöÄ Installation

### Requirements

- Python 3.7+
- PyTorch 2.3.1
- CUDA (for GPU acceleration)

### Setup

1. Clone the repository:
```bash
git clone https://github.com/MPX0222/VisualConcepts4CL.git
cd VisualConcepts4CL
```

2. Install dependencies:
```bash
pip install -r requirements.txt
```

   For concept generation (optional):
```bash
pip install openai scikit-learn nltk
```

3. Download pretrained CLIP models:
   - Create a `pretrained_model/` directory in the project root
   - Download CLIP models and place them in `pretrained_model/`
   - Default path: `pretrained_model/CLIP_ViT-B-16.pt`
   - You can download CLIP models from [OpenAI CLIP](https://github.com/openai/CLIP) or [OpenCLIP](https://github.com/mlfoundations/open_clip)
   - **Note**: Pretrained models are not included in the repository due to their size

4. Download and prepare datasets:
   - Follow the instructions in the [Dataset Preparation](#-dataset-preparation) section
   - Ensure datasets are placed in the expected locations
   - **Note**: Raw dataset files are not included in the repository (see `.gitignore`)

## üìÅ Dataset Preparation

### Important Note

**Datasets are not included in this repository** due to their large size and licensing restrictions. You need to download and prepare the datasets separately. The repository only includes the LLM-generated class descriptions (`datasets/class_descs/`), which are part of the project.

### Supported Datasets

- **CIFAR100**: General object classification (100 classes)
  - Download from: https://www.cs.toronto.edu/~kriz/cifar.html
  - The dataset will be automatically downloaded by torchvision when first used
  
- **ImageNet-R**: ImageNet-Rendition dataset
  - Download from: https://github.com/hendrycks/imagenet-r
  
- **ImageNet100**: Subset of ImageNet (100 classes)
  - Create a subset from ImageNet dataset
  
- **Skin40**: Skin disease classification (40 classes, subset of SD-198)
  - Download from: https://link.springer.com/chapter/10.1007/978-3-319-46466-4_13
  - Expected location: `$HOME/Data/skin40/`
  - Required files: `train_1.txt`, `val_1.txt`, and `images/` directory
  
- **CUB200**: Caltech-UCSD Birds-200-2011 (200 classes)
  - Download from: http://www.vision.caltech.edu/datasets/cub_200_2011/
  
- **Cars196**: Stanford Cars dataset (196 classes)
  - Download from: https://ai.stanford.edu/~jkrause/cars/car_dataset.html
  
- **MedMNIST**: Medical imaging dataset
  - Download from: https://medmnist.com/

### Dataset Structure

#### Raw Dataset Files

Place your downloaded datasets according to the expected paths in each dataset's implementation file. For example:
- Skin40: `$HOME/Data/skin40/` with `train_1.txt`, `val_1.txt`, and `images/` directory
- CIFAR100: Automatically handled by torchvision
- Other datasets: Check the respective dataset class in `datasets/` for expected paths

#### Class Descriptions (Included in Repository)

Each dataset should have its class descriptions stored in `datasets/class_descs/{DATASET_NAME}/`:
- `description_pool.json`: Dictionary mapping class names to lists of LLM-generated descriptions
- `unique_descriptions.txt`: List of unique descriptions used for training

Example structure:
```
datasets/
  class_descs/
    CIFAR100/
      description_pool.json
      unique_descriptions.txt
    Skin40/
      description_pool.json
      unique_descriptions.txt
```

**Note**: The class description files are already included in this repository and do not need to be downloaded separately.

### Generating Class Descriptions (Optional)

If you want to generate your own class descriptions using LLMs, you can use the concept generation tool located in `methods/concept_generation/`. This is an independent module that:

- Generates visual concept descriptions using LLM APIs (OpenAI GPT models)
- Implements a similarity-based filtering mechanism to prevent redundant descriptions
- Supports batch processing and different prompt types (medical, default, CT, etc.)
- Converts description pools to index-based format for efficient storage

**Note**: The pre-generated descriptions in `datasets/class_descs/` are ready to use. You only need to run this tool if you want to generate new descriptions or modify existing ones.

## üéØ Usage

### Training

1. Configure your experiment in a YAML file under `config_yaml/CLIP_Concept/`:
```bash
python main.py --yaml_path config_yaml/CLIP_Concept/cifar100.yaml
```

2. Or specify parameters directly via command line:
```bash
python main.py \
    --yaml_path config_yaml/CLIP_Concept/cifar100.yaml \
    --batch_size 32 \
    --epochs 10 \
    --lr 0.002
```

### Evaluation

Set `is_train = False` in `main.py` to evaluate a trained model:
```python
is_train = False
```

The evaluation script will load checkpoints from the specified save path and evaluate on all tasks.

### Generating Visual Concepts

The `methods/concept_generation/` module is an independent tool for generating LLM-based visual concept descriptions. This tool is used to create the class descriptions that are later used in the main training pipeline.

#### Setup

1. Install additional dependencies:
```bash
pip install openai scikit-learn nltk
```

2. Download NLTK data (required for text preprocessing):
```python
import nltk
nltk.download('stopwords')
nltk.download('punkt_tab')
```

3. Configure API credentials:
   - Edit `description_generator.py` and set your OpenAI API key and base URL
   - Or set environment variables for API configuration

#### Usage

1. Prepare a category file:
   - Create a text file listing all class names (one per line or as a Python list)
   - Example files are available in `methods/concept_generation/datasets/`

2. Run the concept generation:
```bash
cd methods/concept_generation
python main.py
```

3. Configure the generation in `main.py`:
```python
data_info = {
    "path": "datasets/skin8.txt",  # Path to category file
    "need_type": "all",  # "all" for single prompt type, "mixed" for multiple types
    "need_description": {
        "type": "default"  # Prompt type: "default", "medical", or "ct"
    },
    "batch_size": 2  # Number of classes to process per batch
}
```

4. Convert description pool to index format:
```bash
python convert_description_pool.py
```

This will generate:
- `description_pool_v{N}.json`: Description pool after each batch
- `unique_descriptions.txt`: List of all unique descriptions
- `description_pool_indices.json`: Index-based format for efficient storage

#### Features

- **Similarity-based Filtering**: Automatically filters redundant descriptions using TF-IDF and word overlap metrics
- **Batch Processing**: Processes classes in batches to manage API rate limits
- **Multiple Prompt Types**: Supports domain-specific prompts (medical, CT scans, default)
- **Conflict Detection**: Identifies and removes conflicting descriptions within and across categories
- **Automatic Pooling**: Maintains a target number of descriptions per class (default: 5)

#### Output Format

The generated files follow the same format as those in `datasets/class_descs/`:
- `description_pool.json`: `{"class_name": ["concept1", "concept2", ...]}`
- `unique_descriptions.txt`: Python list of all unique descriptions
- `description_pool_indices.json`: Index-based mapping for efficient storage

After generation, you can copy these files to `datasets/class_descs/{DATASET_NAME}/` for use in the main training pipeline.


## ‚öôÔ∏è Configuration

Configuration files are organized in YAML format under `config_yaml/CLIP_Concept/`. Key parameters include:

### Basic Settings
- `method`: Training method (e.g., "CLIP_Concept")
- `increment_type`: Type of incremental learning ("CIL" for class-incremental)
- `increment_steps`: List defining number of classes per task (e.g., `[10, 10, 10, ...]`)

### Model Settings
- `backbone`: Backbone architecture ("CLIP", "OpenCLIP", or "MedCLIP")
- `pretrained_path`: Path to pretrained model weights
- `alpha`: Weight for combining direct and attention-based logits

### Training Settings
- `batch_size`: Batch size for training
- `epochs`: Number of training epochs per task
- `lr`: Learning rate
- `optimizer`: Optimizer type (e.g., "AdamW")
- `scheduler`: Learning rate scheduler (e.g., "Cosine")

### Concept Settings
- `desc_path`: Path to class descriptions file
- `desc_num`: Number of descriptions per class
- `prompt_template`: Template for text prompts (e.g., "a photo of a {}.")
- `lambd`: Weight for attention loss

### Memory Settings (Optional)
- `memory_size`: Total size of replay memory
- `memory_per_class`: Number of exemplars per class
- `sampling_method`: Method for selecting exemplars (e.g., "herding")

### Stage 2 Training (Class-Aware Regularization)
- `ca_epoch`: Number of epochs for stage 2 training
- `ca_lr`: Learning rate for stage 2 training
- `num_sampled_pcls`: Number of samples per class for stage 2
- `ca_logit_norm`: Logit normalization factor (0 to disable)

Example configuration file:
```yaml
basic:
  random_seed: 1993
  version_name: "cifar100_b0i10"
  method: "CLIP_Concept"
  increment_type: "CIL"
  increment_steps: [10, 10, 10, 10, 10, 10, 10, 10, 10, 10]

usual:
  dataset_name: "cifar100"
  backbone: "CLIP"
  pretrained_path: "pretrained_model/CLIP_ViT-B-16.pt"
  batch_size: 32
  epochs: 10
  lr: 0.002
  optimizer: AdamW
  scheduler: Cosine

special:
  desc_path: "./datasets/class_descs/CIFAR100/unique_descriptions.txt"
  prompt_template: "a photo of a {}."
  desc_num: 3
  alpha: 0.5
  ca_epoch: 5
  ca_lr: 0.002
```

## üìÑ License

This project is licensed under the terms specified in the LICENSE file.

## üìß Contact

For questions or issues, please open an issue on GitHub or contact the authors.